{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer with PyTorch and VGG19\n",
    "\n",
    "Welcome to this comprehensive tutorial on Neural Style Transfer! This notebook demonstrates how to combine the content of one image with the artistic style of another using deep learning.\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "- Understanding the Neural Style Transfer algorithm\n",
    "- How VGG19 extracts content and style features\n",
    "- Implementing loss functions for content and style\n",
    "- Optimizing images through gradient descent\n",
    "- Experimenting with different parameters\n",
    "\n",
    "## üéØ Prerequisites\n",
    "\n",
    "- Basic understanding of neural networks\n",
    "- Familiarity with PyTorch\n",
    "- Knowledge of image processing concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing all necessary libraries and our custom modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Custom module imports\n",
    "from neural_style_transfer import NeuralStyleTransfer, VGGFeatureExtractor\n",
    "from utils import (\n",
    "    load_and_preprocess_image, \n",
    "    tensor_to_pil, \n",
    "    save_image, \n",
    "    visualize_comparison,\n",
    "    create_image_grid\n",
    ")\n",
    "from config import Config, PRESETS\n",
    "\n",
    "# Set up matplotlib for better display\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Neural Style Transfer\n",
    "\n",
    "Neural Style Transfer was introduced by Gatys et al. in 2015. The key insight is that:\n",
    "\n",
    "1. **Content Representation**: Higher layers of CNNs capture content information\n",
    "2. **Style Representation**: Correlations between feature maps (Gram matrices) capture style\n",
    "3. **Optimization**: We can optimize a generated image to match both representations\n",
    "\n",
    "### The Algorithm Overview:\n",
    "1. Extract content features from a content image using VGG19\n",
    "2. Extract style features (Gram matrices) from a style image using VGG19\n",
    "3. Start with a random image (or copy of content image)\n",
    "4. Iteratively optimize the image to minimize both content and style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the VGG19 architecture and our feature extraction layers\n",
    "def visualize_vgg_architecture():\n",
    "    \"\"\"Display the VGG19 architecture and highlight our feature extraction layers.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    \n",
    "    # VGG19 layer information\n",
    "    layers = [\n",
    "        ('Input', 'lightblue', 'Input Image\\n224x224x3'),\n",
    "        ('conv1_1', 'lightgreen', 'Conv 3x3, 64\\nReLU\\n224x224x64'),\n",
    "        ('conv1_2', 'lightgray', 'Conv 3x3, 64\\nReLU, MaxPool\\n112x112x64'),\n",
    "        ('conv2_1', 'lightgreen', 'Conv 3x3, 128\\nReLU\\n112x112x128'),\n",
    "        ('conv2_2', 'lightgray', 'Conv 3x3, 128\\nReLU, MaxPool\\n56x56x128'),\n",
    "        ('conv3_1', 'lightgreen', 'Conv 3x3, 256\\nReLU\\n56x56x256'),\n",
    "        ('conv3_2-4', 'lightgray', 'Conv 3x3, 256\\nReLU (x3), MaxPool\\n28x28x256'),\n",
    "        ('conv4_1', 'lightgreen', 'Conv 3x3, 512\\nReLU\\n28x28x512'),\n",
    "        ('conv4_2', 'gold', 'Conv 3x3, 512\\nReLU (CONTENT)\\n28x28x512'),\n",
    "        ('conv4_3-4', 'lightgray', 'Conv 3x3, 512\\nReLU (x2), MaxPool\\n14x14x512'),\n",
    "        ('conv5_1', 'lightgreen', 'Conv 3x3, 512\\nReLU\\n14x14x512'),\n",
    "        ('conv5_2-4', 'lightgray', 'Conv 3x3, 512\\nReLU (x3), MaxPool\\n7x7x512')\n",
    "    ]\n",
    "    \n",
    "    y_positions = np.arange(len(layers))\n",
    "    \n",
    "    for i, (name, color, description) in enumerate(layers):\n",
    "        # Draw rectangle for layer\n",
    "        rect = plt.Rectangle((0, i-0.4), 2, 0.8, facecolor=color, edgecolor='black', linewidth=1)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add layer name\n",
    "        ax.text(1, i, name, ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Add description\n",
    "        ax.text(3, i, description, ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightgreen', label='Style Layers'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='gold', label='Content Layer'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='lightgray', label='Other Layers')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    ax.set_xlim(-0.5, 8)\n",
    "    ax.set_ylim(-0.5, len(layers) - 0.5)\n",
    "    ax.set_title('VGG19 Architecture for Neural Style Transfer', fontsize=16, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä VGG19 Feature Extraction Strategy:\")\n",
    "    print(\"‚Ä¢ Style Layers (Green): conv1_1, conv2_1, conv3_1, conv4_1, conv5_1\")\n",
    "    print(\"‚Ä¢ Content Layer (Gold): conv4_2\")\n",
    "    print(\"‚Ä¢ Style is captured by Gram matrices of feature correlations\")\n",
    "    print(\"‚Ä¢ Content is captured by direct feature comparison\")\n",
    "\n",
    "visualize_vgg_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the Model and Device\n",
    "\n",
    "Let's set up our Neural Style Transfer model and check our computational resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device and model\n",
    "device = Config.get_device(prefer_gpu=True)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Create directories for our outputs\n",
    "Config.create_directories()\n",
    "\n",
    "# Initialize the Neural Style Transfer model\n",
    "nst = NeuralStyleTransfer(device=device)\n",
    "print(\"\\n‚úÖ Neural Style Transfer model initialized!\")\n",
    "\n",
    "# Display current configuration\n",
    "print(\"\\nüîß Current Configuration:\")\n",
    "for key, value in Config.get_default_params().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Visualize Example Images\n",
    "\n",
    "Let's load our example content and style images to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if example images exist, if not we'll create simple ones\n",
    "content_path = \"examples/content_mountain.svg\"\n",
    "style_path = \"examples/style_van_gogh.svg\"\n",
    "\n",
    "def create_sample_images():\n",
    "    \"\"\"Create sample images if examples don't exist.\"\"\"\n",
    "    \n",
    "    # Create a simple content image\n",
    "    content_img = Image.new('RGB', (400, 300), color='skyblue')\n",
    "    # Add some simple shapes\n",
    "    from PIL import ImageDraw\n",
    "    draw = ImageDraw.Draw(content_img)\n",
    "    \n",
    "    # Mountains\n",
    "    draw.polygon([(0, 200), (100, 100), (200, 150), (300, 80), (400, 120), (400, 300), (0, 300)], fill='gray')\n",
    "    # Sun\n",
    "    draw.ellipse([320, 40, 360, 80], fill='yellow')\n",
    "    # Lake\n",
    "    draw.ellipse([100, 220, 300, 250], fill='blue')\n",
    "    \n",
    "    content_img.save('temp_content.jpg')\n",
    "    \n",
    "    # Create a simple style image with patterns\n",
    "    style_img = Image.new('RGB', (400, 300), color='purple')\n",
    "    draw = ImageDraw.Draw(style_img)\n",
    "    \n",
    "    # Create swirl-like patterns\n",
    "    for i in range(0, 400, 20):\n",
    "        for j in range(0, 300, 20):\n",
    "            color = (255 - i//2, 100 + j//3, 150 + i//3)\n",
    "            draw.ellipse([i, j, i+15, j+15], fill=color)\n",
    "    \n",
    "    style_img.save('temp_style.jpg')\n",
    "    \n",
    "    return 'temp_content.jpg', 'temp_style.jpg'\n",
    "\n",
    "# Try to load example images, create simple ones if they don't exist\n",
    "try:\n",
    "    if os.path.exists(content_path) and os.path.exists(style_path):\n",
    "        # Convert SVG to PIL Image for display (simplified)\n",
    "        print(\"üìÅ Using example SVG images\")\n",
    "        content_display_path = content_path\n",
    "        style_display_path = style_path\n",
    "    else:\n",
    "        print(\"üìÅ Creating sample images for demonstration\")\n",
    "        content_display_path, style_display_path = create_sample_images()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Creating fallback images: {e}\")\n",
    "    content_display_path, style_display_path = create_sample_images()\n",
    "\n",
    "# Load images for processing\n",
    "print(\"\\nüñºÔ∏è Loading and preprocessing images...\")\n",
    "image_size = 512\n",
    "\n",
    "if content_display_path.endswith('.svg'):\n",
    "    # For SVG files, we'll need to render them first\n",
    "    print(\"Note: SVG images detected. In a real scenario, you'd convert these to raster images first.\")\n",
    "    print(\"For this demo, we'll create sample raster images.\")\n",
    "    content_display_path, style_display_path = create_sample_images()\n",
    "\n",
    "content_tensor = load_and_preprocess_image(content_display_path, target_size=image_size, device=device)\n",
    "style_tensor = load_and_preprocess_image(style_display_path, target_size=image_size, device=device)\n",
    "\n",
    "# Display the input images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(tensor_to_pil(content_tensor))\n",
    "axes[0].set_title('Content Image', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(tensor_to_pil(style_tensor))\n",
    "axes[1].set_title('Style Image', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Images loaded successfully!\")\n",
    "print(f\"Content image shape: {content_tensor.shape}\")\n",
    "print(f\"Style image shape: {style_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploring Feature Extraction\n",
    "\n",
    "Let's examine how VGG19 extracts features and understand what the network \"sees\" at different layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from both images\n",
    "print(\"üîç Extracting features from VGG19...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Normalize images for VGG\n",
    "    content_features = nst.feature_extractor(nst.normalize_vgg(content_tensor))\n",
    "    style_features = nst.feature_extractor(nst.normalize_vgg(style_tensor))\n",
    "\n",
    "print(\"\\nüìä Feature Map Dimensions:\")\n",
    "for layer_name in nst.feature_extractor.content_layers + nst.feature_extractor.style_layers:\n",
    "    if layer_name in content_features:\n",
    "        shape = content_features[layer_name].shape\n",
    "        print(f\"  {layer_name}: {shape} ({shape[1]} channels, {shape[2]}x{shape[3]} spatial)\")\n",
    "\n",
    "def visualize_feature_maps(features, layer_name, title, max_channels=16):\n",
    "    \"\"\"Visualize feature maps from a specific layer.\"\"\"\n",
    "    if layer_name not in features:\n",
    "        print(f\"Layer {layer_name} not found\")\n",
    "        return\n",
    "    \n",
    "    feature_map = features[layer_name].squeeze(0)  # Remove batch dimension\n",
    "    num_channels = min(feature_map.shape[0], max_channels)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    fig.suptitle(f'{title} - {layer_name} Feature Maps', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(16):\n",
    "        row, col = i // 4, i % 4\n",
    "        if i < num_channels:\n",
    "            # Normalize feature map for visualization\n",
    "            fm = feature_map[i].cpu().numpy()\n",
    "            fm = (fm - fm.min()) / (fm.max() - fm.min() + 1e-8)\n",
    "            \n",
    "            axes[row, col].imshow(fm, cmap='viridis')\n",
    "            axes[row, col].set_title(f'Channel {i+1}', fontsize=10)\n",
    "        else:\n",
    "            axes[row, col].axis('off')\n",
    "        axes[row, col].set_xticks([])\n",
    "        axes[row, col].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature maps from different layers\n",
    "print(\"\\nüé® Content Features (conv4_2):\")\n",
    "visualize_feature_maps(content_features, 'conv4_2', 'Content Image')\n",
    "\n",
    "print(\"\\nüé≠ Style Features (conv1_1):\")\n",
    "visualize_feature_maps(style_features, 'conv1_1', 'Style Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Gram Matrices for Style\n",
    "\n",
    "The Gram matrix captures style by measuring correlations between different feature channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_style_transfer import GramMatrix\n",
    "\n",
    "def visualize_gram_matrices(features, layers, title_prefix):\n",
    "    \"\"\"Visualize Gram matrices for style representation.\"\"\"\n",
    "    gram_computer = GramMatrix()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(layers), figsize=(15, 3))\n",
    "    if len(layers) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        if layer in features:\n",
    "            # Compute Gram matrix\n",
    "            gram = gram_computer(features[layer])\n",
    "            gram_np = gram.squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Visualize Gram matrix\n",
    "            im = axes[i].imshow(gram_np, cmap='coolwarm', aspect='auto')\n",
    "            axes[i].set_title(f'{title_prefix}\\n{layer}', fontsize=12)\n",
    "            axes[i].set_xlabel('Feature Channel')\n",
    "            axes[i].set_ylabel('Feature Channel')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.suptitle('Gram Matrices (Style Representation)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize Gram matrices for style layers\n",
    "style_layers_subset = ['conv1_1', 'conv2_1', 'conv3_1']  # First three for visualization\n",
    "print(\"üé≠ Style Representation via Gram Matrices:\")\n",
    "visualize_gram_matrices(style_features, style_layers_subset, 'Style Image')\n",
    "\n",
    "# Show difference in Gram matrices\n",
    "print(\"\\nüìà Understanding Gram Matrices:\")\n",
    "print(\"‚Ä¢ Each cell (i,j) represents correlation between feature channels i and j\")\n",
    "print(\"‚Ä¢ Diagonal elements show feature channel variances\")\n",
    "print(\"‚Ä¢ Off-diagonal elements show feature channel covariances\")\n",
    "print(\"‚Ä¢ Different artistic styles produce different correlation patterns\")\n",
    "print(\"‚Ä¢ The Gram matrix is texture/style invariant to spatial location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Style Transfer with Parameter Control\n",
    "\n",
    "Now let's create an interactive widget to experiment with different parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widgets for parameter control\n",
    "style_weight_slider = widgets.FloatLogSlider(\n",
    "    value=1000000,\n",
    "    base=10,\n",
    "    min=3,  # 10^3 = 1000\n",
    "    max=7,  # 10^7 = 10000000\n",
    "    step=0.1,\n",
    "    description='Style Weight:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "content_weight_slider = widgets.FloatSlider(\n",
    "    value=1.0,\n",
    "    min=0.1,\n",
    "    max=10.0,\n",
    "    step=0.1,\n",
    "    description='Content Weight:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "steps_slider = widgets.IntSlider(\n",
    "    value=200,  # Reduced for demo purposes\n",
    "    min=50,\n",
    "    max=500,\n",
    "    step=50,\n",
    "    description='Steps:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "learning_rate_slider = widgets.FloatSlider(\n",
    "    value=0.01,\n",
    "    min=0.001,\n",
    "    max=0.1,\n",
    "    step=0.001,\n",
    "    description='Learning Rate:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "preset_dropdown = widgets.Dropdown(\n",
    "    options=['custom'] + list(PRESETS.keys()),\n",
    "    value='balanced',\n",
    "    description='Preset:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='üé® Run Style Transfer',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def update_sliders_from_preset(change):\n",
    "    \"\"\"Update sliders when preset is changed.\"\"\"\n",
    "    preset_name = change['new']\n",
    "    if preset_name in PRESETS:\n",
    "        preset = PRESETS[preset_name]\n",
    "        content_weight_slider.value = preset['content_weight']\n",
    "        style_weight_slider.value = preset['style_weight']\n",
    "        steps_slider.value = min(preset['steps'], 500)  # Cap at 500 for demo\n",
    "\n",
    "preset_dropdown.observe(update_sliders_from_preset, names='value')\n",
    "\n",
    "def run_style_transfer(button):\n",
    "    \"\"\"Run style transfer with current parameters.\"\"\"\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        print(\"üé® Starting Neural Style Transfer...\")\n",
    "        print(f\"Parameters: Content={content_weight_slider.value}, Style={style_weight_slider.value}\")\n",
    "        print(f\"Steps: {steps_slider.value}, Learning Rate: {learning_rate_slider.value}\")\n",
    "        \n",
    "        # Update model weights\n",
    "        nst.set_loss_weights(\n",
    "            content_weight=content_weight_slider.value,\n",
    "            style_weight=style_weight_slider.value\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Run style transfer\n",
    "            stylized_tensor = nst.transfer_style(\n",
    "                content_image=content_tensor,\n",
    "                style_image=style_tensor,\n",
    "                num_steps=steps_slider.value,\n",
    "                learning_rate=learning_rate_slider.value,\n",
    "                show_progress=True\n",
    "            )\n",
    "            \n",
    "            # Display results\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            axes[0].imshow(tensor_to_pil(content_tensor))\n",
    "            axes[0].set_title('Content Image', fontsize=12, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(tensor_to_pil(style_tensor))\n",
    "            axes[1].set_title('Style Image', fontsize=12, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(tensor_to_pil(stylized_tensor))\n",
    "            axes[2].set_title('Stylized Result', fontsize=12, fontweight='bold')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"‚úÖ Style transfer completed successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during style transfer: {e}\")\n",
    "\n",
    "run_button.on_click(run_style_transfer)\n",
    "\n",
    "# Display the interactive interface\n",
    "print(\"üéõÔ∏è Interactive Neural Style Transfer Control Panel\")\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üé® Neural Style Transfer Parameters</h3>\"),\n",
    "        preset_dropdown,\n",
    "        content_weight_slider,\n",
    "        style_weight_slider,\n",
    "        steps_slider,\n",
    "        learning_rate_slider,\n",
    "        run_button,\n",
    "        output_area\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preset Comparison\n",
    "\n",
    "Let's compare different preset configurations to understand their effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_presets(preset_list=['quick', 'balanced', 'more_style'], steps_override=100):\n",
    "    \"\"\"Compare different presets side by side.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(f\"üîÑ Comparing {len(preset_list)} presets...\")\n",
    "    \n",
    "    for preset_name in preset_list:\n",
    "        if preset_name not in PRESETS:\n",
    "            print(f\"Preset '{preset_name}' not found, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        preset = PRESETS[preset_name]\n",
    "        print(f\"\\nüé® Running preset: {preset_name}\")\n",
    "        print(f\"   {preset['description']}\")\n",
    "        print(f\"   Content Weight: {preset['content_weight']}, Style Weight: {preset['style_weight']}\")\n",
    "        \n",
    "        # Update model weights\n",
    "        nst.set_loss_weights(\n",
    "            content_weight=preset['content_weight'],\n",
    "            style_weight=preset['style_weight']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Run style transfer with reduced steps for comparison\n",
    "            stylized = nst.transfer_style(\n",
    "                content_image=content_tensor,\n",
    "                style_image=style_tensor,\n",
    "                num_steps=steps_override,\n",
    "                learning_rate=0.01,\n",
    "                show_progress=False\n",
    "            )\n",
    "            \n",
    "            results[preset_name] = stylized\n",
    "            print(f\"   ‚úÖ Completed!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "    \n",
    "    # Display comparison\n",
    "    if results:\n",
    "        n_results = len(results)\n",
    "        fig, axes = plt.subplots(2, max(3, n_results), figsize=(4*max(3, n_results), 8))\n",
    "        \n",
    "        # Top row: Original images\n",
    "        axes[0, 0].imshow(tensor_to_pil(content_tensor))\n",
    "        axes[0, 0].set_title('Content Image', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        axes[0, 1].imshow(tensor_to_pil(style_tensor))\n",
    "        axes[0, 1].set_title('Style Image', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Hide extra axes in top row\n",
    "        for i in range(2, max(3, n_results)):\n",
    "            axes[0, i].axis('off')\n",
    "        \n",
    "        # Bottom row: Results\n",
    "        for i, (preset_name, result) in enumerate(results.items()):\n",
    "            axes[1, i].imshow(tensor_to_pil(result))\n",
    "            axes[1, i].set_title(f'{preset_name.title()}\\n{PRESETS[preset_name][\"description\"]}', \n",
    "                               fontsize=10, fontweight='bold')\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        # Hide extra axes in bottom row\n",
    "        for i in range(len(results), max(3, n_results)):\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Preset Comparison Results', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüìä Comparison Analysis:\")\n",
    "        print(\"‚Ä¢ 'quick': Fast processing, basic quality\")\n",
    "        print(\"‚Ä¢ 'balanced': Good balance of content and style\")\n",
    "        print(\"‚Ä¢ 'more_style': Emphasizes artistic style over content preservation\")\n",
    "    else:\n",
    "        print(\"‚ùå No successful results to display\")\n",
    "\n",
    "# Run the comparison\n",
    "compare_presets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Understanding Loss Functions\n",
    "\n",
    "Let's dive deeper into how the loss functions work and track them during optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_style_transfer_with_loss_tracking(content_img, style_img, num_steps=100):\n",
    "    \"\"\"Run style transfer while tracking loss components.\"\"\"\n",
    "    \n",
    "    # Reset model to balanced settings\n",
    "    nst.set_loss_weights(content_weight=1.0, style_weight=1000000.0)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        content_features = nst.feature_extractor(nst.normalize_vgg(content_img))\n",
    "        style_features = nst.feature_extractor(nst.normalize_vgg(style_img))\n",
    "    \n",
    "    # Create loss modules\n",
    "    content_losses, style_losses = nst.create_loss_modules(content_features, style_features)\n",
    "    \n",
    "    # Initialize generated image\n",
    "    generated_image = content_img.clone()\n",
    "    generated_image.requires_grad_(True)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.LBFGS([generated_image], lr=0.01)\n",
    "    \n",
    "    # Track losses\n",
    "    total_losses = []\n",
    "    content_losses_track = []\n",
    "    style_losses_track = []\n",
    "    step_count = [0]\n",
    "    \n",
    "    def closure():\n",
    "        step_count[0] += 1\n",
    "        generated_image.data.clamp_(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Extract features from generated image\n",
    "        generated_features = nst.feature_extractor(nst.normalize_vgg(generated_image))\n",
    "        \n",
    "        # Compute losses\n",
    "        total_loss, content_loss, style_loss = nst.compute_total_loss(\n",
    "            generated_features, content_losses, style_losses\n",
    "        )\n",
    "        \n",
    "        # Track losses\n",
    "        if step_count[0] % 10 == 0:  # Track every 10 steps\n",
    "            total_losses.append(total_loss.item())\n",
    "            content_losses_track.append(content_loss)\n",
    "            style_losses_track.append(style_loss)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        return total_loss\n",
    "    \n",
    "    print(f\"üèÉ Running {num_steps} optimization steps with loss tracking...\")\n",
    "    \n",
    "    # Run optimization\n",
    "    progress_bar = tqdm(range(num_steps), desc=\"Optimizing\")\n",
    "    for i in progress_bar:\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        if len(total_losses) > 0:\n",
    "            progress_bar.set_postfix({\n",
    "                'Total Loss': f'{total_losses[-1]:.2e}',\n",
    "                'Content': f'{content_losses_track[-1]:.2e}',\n",
    "                'Style': f'{style_losses_track[-1]:.2e}'\n",
    "            })\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_image.clamp_(0, 1)\n",
    "    \n",
    "    return generated_image.detach(), total_losses, content_losses_track, style_losses_track\n",
    "\n",
    "# Run detailed style transfer\n",
    "result, total_losses, content_losses, style_losses = detailed_style_transfer_with_loss_tracking(\n",
    "    content_tensor, style_tensor, num_steps=150\n",
    ")\n",
    "\n",
    "# Plot loss curves\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "steps_axis = np.arange(0, len(total_losses) * 10, 10)\n",
    "\n",
    "ax1.plot(steps_axis, total_losses, 'b-', linewidth=2, label='Total Loss')\n",
    "ax1.set_title('Total Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Optimization Steps')\n",
    "ax1.set_ylabel('Loss Value')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(steps_axis, content_losses, 'g-', linewidth=2, label='Content Loss')\n",
    "ax2.plot(steps_axis, style_losses, 'r-', linewidth=2, label='Style Loss')\n",
    "ax2.set_title('Content vs Style Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Optimization Steps')\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# Show result\n",
    "ax3.imshow(tensor_to_pil(content_tensor))\n",
    "ax3.set_title('Original Content', fontsize=12, fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "ax4.imshow(tensor_to_pil(result))\n",
    "ax4.set_title('Stylized Result', fontsize=12, fontweight='bold')\n",
    "ax4.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Loss Analysis:\")\n",
    "print(f\"‚Ä¢ Initial Total Loss: {total_losses[0]:.2e}\")\n",
    "print(f\"‚Ä¢ Final Total Loss: {total_losses[-1]:.2e}\")\n",
    "print(f\"‚Ä¢ Loss Reduction: {(total_losses[0] - total_losses[-1]) / total_losses[0] * 100:.1f}%\")\n",
    "print(f\"‚Ä¢ Final Content Loss: {content_losses[-1]:.2e}\")\n",
    "print(f\"‚Ä¢ Final Style Loss: {style_losses[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Experiments\n",
    "\n",
    "### Experiment 1: Effect of Different Style Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_weight_experiment():\n",
    "    \"\"\"Experiment with different style weights to show their effects.\"\"\"\n",
    "    \n",
    "    style_weights = [10000, 100000, 1000000, 10000000]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"üß™ Experimenting with different style weights...\")\n",
    "    \n",
    "    for weight in style_weights:\n",
    "        print(f\"\\nüé® Testing style weight: {weight:,}\")\n",
    "        \n",
    "        nst.set_loss_weights(content_weight=1.0, style_weight=weight)\n",
    "        \n",
    "        try:\n",
    "            result = nst.transfer_style(\n",
    "                content_image=content_tensor,\n",
    "                style_image=style_tensor,\n",
    "                num_steps=100,  # Reduced for experiment\n",
    "                learning_rate=0.01,\n",
    "                show_progress=False\n",
    "            )\n",
    "            results[weight] = result\n",
    "            print(f\"   ‚úÖ Complete\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed: {e}\")\n",
    "    \n",
    "    # Display results\n",
    "    if results:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (weight, result) in enumerate(results.items()):\n",
    "            axes[i].imshow(tensor_to_pil(result))\n",
    "            axes[i].set_title(f'Style Weight: {weight:,}', fontsize=12, fontweight='bold')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Effect of Different Style Weights', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nüìä Style Weight Analysis:\")\n",
    "        print(\"‚Ä¢ Lower weights (10K): More content preservation, less style\")\n",
    "        print(\"‚Ä¢ Medium weights (100K-1M): Balanced style transfer\")\n",
    "        print(\"‚Ä¢ Higher weights (10M): Strong style application, content may be lost\")\n",
    "\n",
    "style_weight_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Progressive Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_style_transfer():\n",
    "    \"\"\"Show the progression of style transfer over time.\"\"\"\n",
    "    \n",
    "    print(\"üé¨ Creating progressive style transfer visualization...\")\n",
    "    \n",
    "    # Reset to balanced settings\n",
    "    nst.set_loss_weights(content_weight=1.0, style_weight=1000000.0)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        content_features = nst.feature_extractor(nst.normalize_vgg(content_tensor))\n",
    "        style_features = nst.feature_extractor(nst.normalize_vgg(style_tensor))\n",
    "    \n",
    "    # Create loss modules\n",
    "    content_losses, style_losses = nst.create_loss_modules(content_features, style_features)\n",
    "    \n",
    "    # Initialize generated image\n",
    "    generated_image = content_tensor.clone()\n",
    "    generated_image.requires_grad_(True)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.LBFGS([generated_image], lr=0.01)\n",
    "    \n",
    "    # Capture intermediate results\n",
    "    snapshots = []\n",
    "    snapshot_steps = [0, 25, 50, 100, 150, 200]\n",
    "    step_count = [0]\n",
    "    \n",
    "    def closure():\n",
    "        step_count[0] += 1\n",
    "        generated_image.data.clamp_(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Capture snapshots\n",
    "        if step_count[0] in snapshot_steps:\n",
    "            with torch.no_grad():\n",
    "                snapshots.append((step_count[0], generated_image.clone()))\n",
    "        \n",
    "        # Extract features from generated image\n",
    "        generated_features = nst.feature_extractor(nst.normalize_vgg(generated_image))\n",
    "        \n",
    "        # Compute losses\n",
    "        total_loss, _, _ = nst.compute_total_loss(\n",
    "            generated_features, content_losses, style_losses\n",
    "        )\n",
    "        \n",
    "        total_loss.backward()\n",
    "        return total_loss\n",
    "    \n",
    "    # Initial snapshot\n",
    "    snapshots.append((0, generated_image.clone()))\n",
    "    \n",
    "    # Run optimization\n",
    "    for i in tqdm(range(200), desc=\"Progressive Transfer\"):\n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_image.clamp_(0, 1)\n",
    "    \n",
    "    # Display progression\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (step, snapshot) in enumerate(snapshots[:6]):\n",
    "        axes[i].imshow(tensor_to_pil(snapshot))\n",
    "        axes[i].set_title(f'Step {step}', fontsize=14, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Progressive Style Transfer Evolution', fontsize=18, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüé¨ Progressive Analysis:\")\n",
    "    print(\"‚Ä¢ Step 0: Original content image\")\n",
    "    print(\"‚Ä¢ Steps 25-50: Initial style patterns appear\")\n",
    "    print(\"‚Ä¢ Steps 100-150: Style becomes more pronounced\")\n",
    "    print(\"‚Ä¢ Step 200: Final stylized result with refined details\")\n",
    "\n",
    "progressive_style_transfer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Analysis and Tips\n",
    "\n",
    "Let's analyze the performance characteristics and provide optimization tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def performance_analysis():\n",
    "    \"\"\"Analyze performance characteristics of different configurations.\"\"\"\n",
    "    \n",
    "    print(\"‚ö° Performance Analysis\\n\")\n",
    "    \n",
    "    # Test different image sizes\n",
    "    image_sizes = [256, 512, 768]\n",
    "    size_times = []\n",
    "    \n",
    "    print(\"üìè Testing different image sizes:\")\n",
    "    for size in image_sizes:\n",
    "        print(f\"\\n  Testing {size}x{size} image...\")\n",
    "        \n",
    "        # Resize test images\n",
    "        test_content = load_and_preprocess_image(content_display_path, target_size=size, device=device)\n",
    "        test_style = load_and_preprocess_image(style_display_path, target_size=size, device=device)\n",
    "        \n",
    "        nst.set_loss_weights(content_weight=1.0, style_weight=1000000.0)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = nst.transfer_style(\n",
    "                content_image=test_content,\n",
    "                style_image=test_style,\n",
    "                num_steps=50,  # Reduced for timing\n",
    "                learning_rate=0.01,\n",
    "                show_progress=False\n",
    "            )\n",
    "            elapsed_time = time.time() - start_time\n",
    "            size_times.append(elapsed_time)\n",
    "            print(f\"    ‚úÖ Time: {elapsed_time:.2f} seconds\")\n",
    "            \n",
    "            # Memory usage\n",
    "            if torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "                print(f\"    üíæ GPU Memory: {memory_used:.2f} GB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Failed: {e}\")\n",
    "            size_times.append(None)\n",
    "    \n",
    "    # Test different step counts\n",
    "    step_counts = [50, 100, 200, 500]\n",
    "    step_times = []\n",
    "    \n",
    "    print(\"\\nüèÉ Testing different step counts (512x512 image):\")\n",
    "    for steps in step_counts:\n",
    "        print(f\"\\n  Testing {steps} steps...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = nst.transfer_style(\n",
    "                content_image=content_tensor,\n",
    "                style_image=style_tensor,\n",
    "                num_steps=steps,\n",
    "                learning_rate=0.01,\n",
    "                show_progress=False\n",
    "            )\n",
    "            elapsed_time = time.time() - start_time\n",
    "            step_times.append(elapsed_time)\n",
    "            print(f\"    ‚úÖ Time: {elapsed_time:.2f} seconds\")\n",
    "            print(f\"    üìä Time per step: {elapsed_time/steps:.3f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Failed: {e}\")\n",
    "            step_times.append(None)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Image size vs time\n",
    "    valid_size_data = [(size, time) for size, time in zip(image_sizes, size_times) if time is not None]\n",
    "    if valid_size_data:\n",
    "        sizes, times = zip(*valid_size_data)\n",
    "        ax1.plot(sizes, times, 'bo-', linewidth=2, markersize=8)\n",
    "        ax1.set_xlabel('Image Size (pixels)')\n",
    "        ax1.set_ylabel('Time (seconds)')\n",
    "        ax1.set_title('Processing Time vs Image Size\\n(50 steps)', fontsize=12, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Steps vs time\n",
    "    valid_step_data = [(steps, time) for steps, time in zip(step_counts, step_times) if time is not None]\n",
    "    if valid_step_data:\n",
    "        steps, times = zip(*valid_step_data)\n",
    "        ax2.plot(steps, times, 'ro-', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Number of Steps')\n",
    "        ax2.set_ylabel('Time (seconds)')\n",
    "        ax2.set_title('Processing Time vs Steps\\n(512x512 image)', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Performance Tips:\")\n",
    "    print(\"‚Ä¢ GPU acceleration provides 10-50x speedup over CPU\")\n",
    "    print(\"‚Ä¢ Processing time scales roughly quadratically with image size\")\n",
    "    print(\"‚Ä¢ Processing time scales linearly with number of steps\")\n",
    "    print(\"‚Ä¢ For experimentation: use 256-512px images with 100-200 steps\")\n",
    "    print(\"‚Ä¢ For high quality: use 768-1024px images with 500-1000 steps\")\n",
    "    print(\"‚Ä¢ Memory usage scales with image size squared\")\n",
    "    print(\"‚Ä¢ LBFGS optimizer is more memory-efficient than Adam for this task\")\n",
    "\n",
    "performance_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed a comprehensive exploration of Neural Style Transfer. Let's summarize what you've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary visualization\n",
    "def create_summary():\n",
    "    \"\"\"Create a summary of the Neural Style Transfer process.\"\"\"\n",
    "    \n",
    "    print(\"üìö Neural Style Transfer - Complete Tutorial Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary_points = [\n",
    "        \"üß† Theoretical Understanding\",\n",
    "        \"   ‚Ä¢ Neural Style Transfer combines content and style representations\",\n",
    "        \"   ‚Ä¢ VGG19 extracts hierarchical features from images\",\n",
    "        \"   ‚Ä¢ Content captured by direct feature comparison (conv4_2)\",\n",
    "        \"   ‚Ä¢ Style captured by Gram matrices of feature correlations\",\n",
    "        \"\",\n",
    "        \"üîß Implementation Details\",\n",
    "        \"   ‚Ä¢ PyTorch-based implementation with pre-trained VGG19\",\n",
    "        \"   ‚Ä¢ LBFGS optimizer for image optimization\",\n",
    "        \"   ‚Ä¢ Combined loss function: Œ±*L_content + Œ≤*L_style\",\n",
    "        \"   ‚Ä¢ Iterative optimization from content image\",\n",
    "        \"\",\n",
    "        \"‚öôÔ∏è Parameter Effects\",\n",
    "        \"   ‚Ä¢ Content weight (Œ±): Higher values preserve more content\",\n",
    "        \"   ‚Ä¢ Style weight (Œ≤): Higher values apply more style\",\n",
    "        \"   ‚Ä¢ Steps: More steps = better quality, longer processing\",\n",
    "        \"   ‚Ä¢ Image size: Larger = higher quality, more memory/time\",\n",
    "        \"\",\n",
    "        \"üöÄ Performance Optimization\",\n",
    "        \"   ‚Ä¢ GPU acceleration essential for practical use\",\n",
    "        \"   ‚Ä¢ Balance image size vs processing time\",\n",
    "        \"   ‚Ä¢ Use presets for common scenarios\",\n",
    "        \"   ‚Ä¢ Monitor memory usage for large images\",\n",
    "        \"\",\n",
    "        \"üéØ Next Steps\",\n",
    "        \"   ‚Ä¢ Experiment with different style images\",\n",
    "        \"   ‚Ä¢ Try real-world content images\",\n",
    "        \"   ‚Ä¢ Explore fast style transfer methods\",\n",
    "        \"   ‚Ä¢ Implement video style transfer\",\n",
    "        \"   ‚Ä¢ Study perceptual loss functions\"\n",
    "    ]\n",
    "    \n",
    "    for point in summary_points:\n",
    "        print(point)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ Thank you for completing this Neural Style Transfer tutorial!\")\n",
    "    print(\"üåê Explore more at: github.com/your-repo/neural-style-transfer\")\n",
    "\n",
    "create_summary()\n",
    "\n",
    "# Display final example with best settings\n",
    "print(\"\\nüé® Final Demonstration - High Quality Result:\")\n",
    "\n",
    "# Reset to high quality settings\n",
    "nst.set_loss_weights(content_weight=1.0, style_weight=1000000.0)\n",
    "\n",
    "final_result = nst.transfer_style(\n",
    "    content_image=content_tensor,\n",
    "    style_image=style_tensor,\n",
    "    num_steps=300,\n",
    "    learning_rate=0.01,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Create final comparison\n",
    "visualize_comparison(\n",
    "    content_tensor, \n",
    "    style_tensor, \n",
    "    final_result,\n",
    "    figsize=(18, 6)\n",
    ")\n",
    "\n",
    "# Save the final result\n",
    "save_image(final_result, \"outputs/notebook_final_result.jpg\")\n",
    "print(\"üíæ Final result saved to: outputs/notebook_final_result.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Additional Resources\n",
    "\n",
    "### Papers and Research\n",
    "- **Original Paper**: Gatys et al. \"A Neural Algorithm of Artistic Style\" (2015)\n",
    "- **Fast Style Transfer**: Johnson et al. \"Perceptual Losses for Real-Time Style Transfer\" (2016)\n",
    "- **Improved Quality**: Li et al. \"Demystifying Neural Style Transfer\" (2017)\n",
    "\n",
    "### Extensions and Improvements\n",
    "- **AdaIN**: Huang & Belongie \"Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization\" (2017)\n",
    "- **WCT**: Li et al. \"Universal Style Transfer via Feature Transforms\" (2017)\n",
    "- **Avatar-Net**: Sheng et al. \"Avatar-Net: Multi-scale Zero-shot Style Transfer\" (2018)\n",
    "\n",
    "### Practical Applications\n",
    "- Mobile apps (Prisma, DeepArt)\n",
    "- Video style transfer\n",
    "- Interactive art creation\n",
    "- Augmented reality filters\n",
    "- Digital art generation\n",
    "\n",
    "### Try These Experiments\n",
    "1. **Different Style Images**: Try famous paintings, textures, or abstract art\n",
    "2. **Video Style Transfer**: Apply to video frames for animated effects\n",
    "3. **Multiple Styles**: Blend multiple style images\n",
    "4. **Semantic Style Transfer**: Apply different styles to different regions\n",
    "5. **3D Style Transfer**: Extend to 3D scenes and objects\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook provided a comprehensive introduction to Neural Style Transfer. The techniques learned here form the foundation for many modern AI art applications. Happy experimenting!* üé®‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
